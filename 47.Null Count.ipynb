{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea782059-2612-4b7b-a06d-bdf66debd87b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_student = [\n",
    "    (\"Michael\", \"Science\",     80,  \"P\",  90),\n",
    "    (\"Nancy\",   \"Mathematics\", 90,  \"P\",  None),\n",
    "    (\"David\",   \"English\",     20,  \"F\",  80),\n",
    "    (\"John\",    \"Science\",     None,\"F\",  None),\n",
    "    (\"Martin\",  \"Mathematics\", None, None,70),\n",
    "    (None,      None,          None, None, None)\n",
    "]\n",
    "\n",
    "Schema = [\"name\", \"Subject\", \"Mark\", \"Status\", \"Attendance\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data_student, schema=Schema)\n",
    "display(df)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b51000e-929a-4cc6-b25b-139d6040c821",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "null_count_row = df.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c)\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "display(null_count_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df5b6cd9-5e19-433e-9740-8b2f9115e943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Faster method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8fa3313-1c89-4de1-ad69-c5210c11a8d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "exprs = [\n",
    "    count(when(col(c).isNull(), c)).alias(c)\n",
    "    for c in df.columns\n",
    "]\n",
    "\n",
    "row_df = df.select(exprs)\n",
    "\n",
    "# convert columns â†’ rows\n",
    "result = (\n",
    "    row_df\n",
    "    .selectExpr(\"stack({0}, {1}) as (Column, Null_Count)\"\n",
    "                .format(len(df.columns),\n",
    "                        \",\".join([f\"'{c}', `{c}`\" for c in df.columns])))\n",
    ")\n",
    "\n",
    "display(result)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "47.Null Count",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
