{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcf18c94-aefb-4510-9be4-1a0fe240feeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    # John\n",
    "    (\"John\", 4, 1),\n",
    "    (\"John\", 6, 2),\n",
    "    (\"John\", 7, 3),\n",
    "    (\"John\", 9, 7),\n",
    "    (\"John\", 2, 7),\n",
    "\n",
    "    # David\n",
    "    (\"David\", 7, 3),\n",
    "    (\"David\", 5, 2),\n",
    "    (\"David\", 1, 8),\n",
    "    (\"David\", 4, 9),\n",
    "    (\"David\", 7, 4),\n",
    "    (\"David\", 1, 9),\n",
    "\n",
    "    # Mike\n",
    "    (\"Mike\", 3, 1),\n",
    "    (\"Mike\", 9, 2),\n",
    "    (\"Mike\", 1, 3),\n",
    "    (\"Mike\", 6, 5),\n",
    "    (\"Mike\", 2, 8),\n",
    "\n",
    "    # Sarah\n",
    "    (\"Sarah\", 5, 7),\n",
    "    (\"Sarah\", 7, 8),\n",
    "    (\"Sarah\", 8, 5),\n",
    "    (\"Sarah\", 1, 1),\n",
    "\n",
    "    # Kevin\n",
    "    (\"Kevin\", 3, 5),\n",
    "    (\"Kevin\", 5, 9),\n",
    "    (\"Kevin\", 7, 7),\n",
    "\n",
    "    # Emma\n",
    "    (\"Emma\", 2, 6),\n",
    "    (\"Emma\", 4, 4),\n",
    "    (\"Emma\", 6, 2)\n",
    "]\n",
    "\n",
    "schema = [\"Name\", \"Array_1_value\", \"Array_2_value\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "416e0314-5a4a-400b-a349-2b1c6b5500e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "agg_df=df.groupBy(df[\"Name\"]).agg(\n",
    "    collect_list(\"Array_1_value\").alias(\"Array_1\"),\n",
    "        collect_list(\"Array_2_value\").alias(\"Array_2\")\n",
    ")\n",
    "display(agg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2906b996-f14d-4b78-ac7d-91d51555da26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_except\n",
    "df_intersect=agg_df.withColumn(\"Intersect\",array_except(agg_df[\"Array_1\"],agg_df[\"Array_2\"]))\n",
    "display(df_intersect)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "42.Array_except",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
